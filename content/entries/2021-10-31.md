## Schauergeschichten der Analyse

Fröhliches Halloween-Wochenende! Ich muss zugeben, daß alles Gruseliges und Gotisches mich immer interessiert hat und daß ich ziemlich viel krankhafte Neugier besitze. Diese Neigung beschränkt sich nicht nur auf die Horrorfilmen und die Gruselgeschichten sondern gilt sie auch für meine Interesse an der Mathematik. Während der frühesten höheren Mathematikkursen der Sekundarschule und der Universität lernt man viele schöne Theoreme und Methoden, die auf der Voraussetzung "hinreichend gut benommene" Funktionen beruhen. Aber jetzt, wo ich Lehrassistent für einen Rechenverfahrenskurs bin, und zwei Kursen über Real- und Fourieranalyse mache, habe ich verschiedene mathematischen Funktionen begegnet, die ausgesprochen *schlecht* benommen  und sogar abscheulich sind. In diesem Blogeintrag werde ich einige "Ungeheuer" der mathematischen Analyse darstellen, die unseren üblichen numerische Methoden durcheinanderbringen und vor den alle angewandte Mathematiker sich fürchten... und in manchen Fällen, wie man sich vor ihr schützen kann.

Notiz: weil es so viele Beispiele gibt, die ich abbilden wollte, und weil ich so beschäftigt auch mit den Hausaufgaben bin, habe ich mir nicht die Mühe gegeben, Erklärungen für jeden aufzuschreiben. Die Meisten laße ich als Übungen oder Denkanregungen.

### Das Newtonverfahren <a id="toc-section-1" class="toc-section"></a>

Das Newtonverfahren ist ein Algorithmus zur Approximation der Lösung nicht linearen Gleichungen, im Besonderen im Form von

$$f(x)=0$$

wo $f:\mathbb R \to\mathbb R$ "hinreichend gut benommen" ist. Zum Beispiel, wenn $f(x)=x^2-2$, bietet das Newtonverfahren eine Approximation von $\sqrt{2}$. Der Algorithmus läuft folgenderweise: wir definieren eine Folge von Approximationen $x_0,x_1,x_2...$ vermittels der Rekursionsgleichung

$$x_{n+1} = x_n - \frac{f(x_n)}{f'(x_n)}$$

wo $x_0$ eine Schätzung ist, die "hinreichend nah" zur wirklichen Wurzel steht. Graphisch interpretiert ist $x_{n+1}$ die Wurzel der Gerade, die $f(x)$ am besten in der Nähe von $x_n$ approximiert, also der Berührungslinie.

Für die meisten "angenehme" Funktionen hat das Newtonverfahren quadratischer Konvergenzordnung, wie zum Beispiel $f(x) = x^2-2$. In diesem Fall ist die Rekursionsgleichung

$$x_{n+1} = x_n - \frac{x_n^2-2}{2x} = \frac{x_n + \frac{2}{x_n}}{2}$$

Mit der Schätzung $x_0 = 1$ läuft die Folge so:

$$\begin{array}{c|c|c}
\text{Estimate} & \text{Value} & \text{Error} \\
0 & 1 & 4.14 \cdot 10^{-1} \\
1 & 1.5 & 8.57 \cdot 10^{-2} \\
2 & 1.416666... & 2.45 \cdot 10^{-3} \\
3 & 1.414215... & 2.12 \cdot 10^{-6} \\
4 & 1.414214... & 1.59 \cdot 10^{-12} \\
\end{array}$$

Ausgezeichnet! Der Stellenzahl von Genauigkeit ist mit jeder Wiederholung verdoppelt. Betracht mal jetzt die Funktion $f(x) = (x^2-2)^2$, die die gleiche Wurzel $x=\sqrt{2}$ besitzt. Von dieser haben wir die Rekursionsgleichung

$$x_{n+1} = x_n - \frac{(x_n^2-2)^2}{4x_n(x_n^2-2)} = \frac{3x_n+\frac{2}{x_n}}{4}$$

die die folgende Folge hervorbringt:

$$\begin{array}{c|c|c}
\text{Estimate} & \text{Value} & \text{Error} \\
0 & 1 & 4.14 \cdot 10^{-1} \\
1 & 1.25 & 1.64 \cdot 10^{-1} \\
2 & 1.3375 & 7.67 \cdot 10^{-2} \\
3 & 1.376957... & 3.72 \cdot 10^{-2} \\
4 & 1.395837... & 1.83 \cdot 10^{-2} \\
5 & 1.405085... & 9.12 \cdot 10^{-3} \\
\end{array}$$

In diesem Fall die Genauigkeit steigt nur linear. Auch für die Funktionen $f(x) = (x^2-2)^m$ wo $m\in \mathbb N\backslash \{1\}$. Im Allgemeinen funktioniert das Newtonverfahren nicht richtig wann $f$ eine mehrfache Nullstelle hat. Die Genauigkeit kann auch sehr langsamer als linear steigen - z.B. für die Funktion $f(x)=e^{-1/x^2}$, die die Wurzel $x=0$ hat, nimmt der Schätzfehler sehr langsam ab:

<center>![Fig 1](/img/2021-10-31-Fig1.png)</center>

Es gibt auch Funktionen, für die die Nähe von $x_0$ zur wahrhaften Nullstelle gar keine Konvergenz garantiert. Schau mal dieses Monster an:

$$f(x) = e^{-1/x^2}(2 + \cos (1/x))$$

Die hat unendlich viel Extrema in der Nähe von der Nullstelle $x=0$, in der Nähe von denen die Berührungslinie von $f$ fast flach ist. Solange $x_0$ ausreichend nah zu einem Extremwert steht, kann der Schätzfehler von $x_1$ größer als die von $x_0$ sein. Zu allem Übel gibt es auch $x_0$ in jeder Umgebung der Wurzel, für den das Newtonverfahren eine periodische Folge $x_0, x_1, x_0, x_1, ...$ hervorbringt! (Kannst du diese Tatsache analytisch beweisen?)

Tatsächlich ist jeder der obenen Gegenbeispiele ziemlich "gut benommen" - jedes ist sogar $C^\infty$! Also, was genau erfordert das Newtonverfahren von einer Funktion? Der wirkliche Theorem lautet folgendermaßen:

> Wenn $f:\mathbb R \to \mathbb R$ stetig differenzierbar in einer Umgebung von der Nullstelle $x=\alpha$, $f'(\alpha)\ne 0$ und $f''(\alpha)$ existiert, dann konvergiert das Newtonverfahren quadratisch.

### Die Polynominterpolation <a id="toc-section-2" class="toc-section"></a>

Zu jeder Menge von $n$ Punkte $\{(x_j,y_j)\}$ entspricht ein einziges Polynom $P$ $n-1$ -ten Grades, das sie interpoliert, damit $P(x_j) = y_j$ für alle $j=1,2,...,n$, solange alle $x_j$ verschieden sind. Deshalb wenn $f:\mathbb [a,b]\to\mathbb R$ irgendeine Funktion ist, können wir immer ein Polynom $P:[a,b]\to\mathbb R$ $n-1$ -ten Grades auch finden, das an $n$ vorgeschriebenen Punkte $x_1,...,x_n$ gleichwertig wie $f$ sind, sodaß $P(x_j) = f(x_j)$. Unter bestimmten Bedingungen wird $P$ auch nah (doch nicht gleich) zu $f$ sonst überall in $[a,b]$ kommen... aber nicht immer.

Zum Beispiel, im Interval $[-1,1]$, die Funktion $f(x)=e^x$ läßt sich sehr gut von Polynomen interpolieren: mit nur $n=6$ Punkten sinkt der Maximalfehler der Interpolante niedriger als $10^{-5}$ und mit $n=9$ Punkten niedriger als $10^{-8}$. Unten siehst du den Graph des Fehlerwerts $|e^x - P_n(x)|$ im Interval $[-1,1]$ für $n=3,6,9,12$:

<center>![Fig 2](/img/2021-10-31-Fig2.png)</center>

und hier findest du einen Graph des Maximalfehlers für jeder $n$:

<center>![Fig 3](/img/2021-10-31-Fig3.png)</center>

Schön! Aber für andere Funktionen funktioniert diese Methode der Polynominterpolation nicht so gut. Zum Beispiel die Funktion

$$f(x)=\frac{1}{x^2+1}$$

laßt den folgenden Maximalfehlergraph entstehen:

<center>![Fig 4](/img/2021-10-31-Fig4.png)</center>

Es sieht so aus, als ob diese Polynomen auch konvergieren, nur aber ein bisschen langsamer. Betracht mal jetzt die Funktion

$$f(x)=\frac{1}{25x^2+1}$$

die dem folgenden Fehlergraph entspricht:

<center>![Fig 5](/img/2021-10-31-Fig5.png)</center>

Was ist hier passiert? Obwohl diese Funktion ziemlich "gut benommen" ist - also, kontinuierlich und stetig unendlich oft differenzierbar - konvergieren seine Interpolante gar nicht und sowohl schlechter werden als $n$ steigt. Hier findest du ein Graph mit der Funktion $f$ und ihrer Interpolante mit $n=12$:

<center>![Fig 6](/img/2021-10-31-Fig6.png)</center>

Aus irgendeinem Grund schwankelt die Interpolante unbeherrschbar in der Nähe von den Intervallgrenzen. Diese Besonderheit heißt [Runges Phänomen](https://en.wikipedia.org/wiki/Runge's_phenomenon). Was ist hier los? Warum überhaupt gibt diese bestimmte Funktion sich nicht zum Polynominterpolation her?

Vermittels des Satzes von Rolle kann man beweisen, daß wenn $f$ glatt ist, ist der Fehler der Polynominterpolante mit $n+1$ Punkten $x_0<...<x_n$ gleich

$$|f(x)-P_{n}(x)| = \Bigg|\frac{f^{(n+1)}(\xi)}{(n+1)!}\prod_{j=0}^n (x-x_j) \Bigg|$$

wo $\xi$ ein bestimmten Zahl im Intervall $[x_0,x_n]$ ist, solange $x\in [x_0,x_n]$ auch. Wenn $x_0=-1$ und $x_n=1$ und die gleichmäßig verteilt in $[x_0,x_n]$ sind, daraus folgt, daß

$$x_j = -1 + \frac{2j}{n}$$

und damit kann man auch beweisen, daß

$$\Bigg|\prod_{j=0}^n (x-x_j) \Bigg|\leq \frac{n!}{4}\Big(\frac{2}{n}\Big)^{n+1}$$

und deshalb

$$|f(x)-P_{n}(x)| \le \Big(\frac{2}{n}\Big)^{n+1} \frac{|f^{(n+1)}(\xi)|}{n+1}$$

Infolgedessen konvergieren die Interpolanten $P_n$ solange $|f(x)-P_{n}(x)|\to 0$ als $n\to\infty$ für jeder $x\in [x_0,x_n]$, derentwegen würde die Bedingung ausreichen, daß

$$\lim_{n\to\infty}\Big(\frac{2}{n}\Big)^{n+1} \frac{|f^{(n+1)}(\xi)|}{n+1} = 0$$

also, daß $|f^{(n+1)}(\xi)|$ nicht "superschnell" steigt als $n\to\infty$. Mit der Funktion $f(x)=e^x$ ist diese Bedingung ideal, weil in diesem Fall $f^{(n+1)}(x)=e^x$ auch und deshalb

$$e^{-1} \le f^{(n+1)}(\xi)\le e$$

für alle $\xi\in [-1,1]$. Für $f(x) = \frac{1}{1+25x^2}$ steigen $f^{(n+1)}(\xi)$ viel schneller: im Besonderen

$$|f^{(2n)}(0)| = 5^{2n}(2n)!$$

und derentwegen

$$\lim_{n\to\infty}\Big(\frac{2}{2n}\Big)^{2n+1} \frac{5^{2n}(2n)!}{2n+1} = \infty$$

...also, unsere Bedingung wird gar nicht erfüllt. Wir beschließen, daß die Glätte der Funktion $f$ keine Konvergenz der Polynominterpolanten garantiert, weil auch wenn die Ableitungen $f^{(n+1)}$ existieren überall, können die Interpolanten "unkontrollierbar schwankeln" wenn $f^{(n+1)}$ "zu groß" sind.

### Die Fourierreihe <a id="toc-section-3" class="toc-section"></a>

Die Fourierreihe einer periodischen Funktion $f:\mathbb R\to\mathbb R$ ist die reihe

$$\sum_{n\in\mathbb Z} \hat{f}(n) e^{inx}$$

wo die Fourier-Koeffizienten $\hat{f}(n)$ sind

$$\hat{f}(n) = \frac{1}{2\pi} \int_0^{2\pi} f(x)e^{-inx}dx$$

Der grundlegenden Theorem der Fourieranalyse behauptet daß solange $f$ eine "angemessene" periodische Funktion ist, konvergiert seine Fourierreihe gegen sich selbst:

$$f(x) = \sum_{n\in\mathbb Z} \hat{f}(n) e^{inx}$$

Laut der Professorin meines Fourieranalysenkurses war die Hauptfrage der Fourieranalyze lange Zeit "was genau heißt *angemessen*?" Also, welche Beschränkungen wirklich garantieren, daß die Fourierreihe einer Funktion konvergiert? Eine glaubhafte Vermutung: vielleicht kann es sein, daß die Kontinuität punktweise Konvergenz der Fourierreihe garantiert. Erstaunlicherweise aber gar nicht. Tatsächlich habe ich eine solche Grässlichkeit als Abschlussproject für den Kurs erdacht - wenn dieses Thema dich interessiert, [hier findest du einen Entwurf meines Projects](https://franklin.dyer.me/file/fourier-final-project.pdf).

Trotz der Tragik dieser kontraintuitiven Ergebnis gelang es mir zu beweisen, daß alle kontinuierliche Funktionen außer die "schlechteste" konvergente Fourierreihen haben. Nämlich klappt es für all Lipschitz-Funktionen und Hölder-kontinuierliche Funktionen, also, alle Funktionen die nicht "steiler" als eine Potenzfunktion sind. Außerdem klappt es für alle lokalmonoton Funktionen. Deshalb muß jeder Gegenbeispiel "sehr steil" und "grenzenlos wackelnd" in der Nähe von einem Punkt sein.

Wir haben auch das folgende Resultat in meinem Fourieranalysenkurs bewiesen:

> Solange $f:\mathbb R\to\mathbb R$ $2\pi$ -periodisch und stetig differenzierbar ist, konvergiert seine Fourierreihe überall.

Dieses ist aber ein schwächeres Resultat, als das was ich in meinem Projekt demostriere, weil alle differenzierbare Funktionen local Hölder-kontinuierlich sind, aber das Gegenteil ist nicht wahr. Man kann auch beweisen, daß obwohl die Kontinuität kein Konvergenz der Fouriersummen $S_n f$ garantiert, zieht sie etwas schwächer aber trotzdem interessant nach sich:

> Solange $f:\mathbb R\to\mathbb R$ $2\pi$ -periodisch und kontinuierlich ist, konvergieren die *Durchschnitte* ihrer Fouriersummen: $$\lim_{n\to\infty} \frac{S_0 f(x) + ... + S_n f(x)}{n+1} = f(x)$$

### Ausgang <a id="toc-section-4" class="toc-section"></a>

Wir haben hier demonstriert, daß obwohl eine Funktion anscheinend "gut benommen" auf den ersten Blich aussieht, sei es kontinuierlich, differenzierbar, oder glatt, kann es andere Eigenschaften besessen, die die übliche numerische Methoden versauen, z.B.:

- mehrfache Nullstellen
- unendliche Schwankungen in der Nähe eines Punktes
- schnell steigende Ableitungen
- extrem Steilheit

Lass dir das eine Lehre sein... wenn du eine numerische Technik auf einer Funktion das nächste Mal anzuwenden versuchst, Vorsicht! Es kann sein, daß deine Funktion eine von diesen verkleideten Ungeheueren ist... (zwar ist es aber ziemlich unwahrscheinlich.)

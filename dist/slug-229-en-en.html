<html>
<head>
    <title>Some collected cool stats about the Wiener process</title>
    <link rel="stylesheet" type="text/css" href="/css/style.css" />
    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
      processEscapes: true,
      menuSettings: { inTabOrder: false },
      "AssistiveMML": {
         disabled: false,
         styles: {
            ".MJX_Assistive_MathML": {
                position:"absolute!important",
                clip: (MathJax.Hub.Browser.isMSIE && (document.documentMode||0) < 8 ?
                    "rect(1px 1px 1px 1px)" : "rect(1px, 1px, 1px, 1px)"),
                padding: "1px 0 0 0!important",
                border: "0!important",
                height: "1px!important",
                width: "1px!important",
                overflow: "hidden!important",
                display:"block!important"
            }
        }
      }
    });
    </script>
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no, type=text/html" charset="UTF-8">
    <link rel="stylesheet" href="/css/style.css">
    <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML' async></script>
    <!-- Cloudflare Web Analytics -->
    <script defer src='https://static.cloudflareinsights.com/beacon.min.js' data-cf-beacon='{"token": "5b8562dcfaa5413ba2aea8294b9d585e"}'></script><!-- End Cloudflare Web Analytics -->
</head>
<body>
    <h2>Some collected cool stats about the Wiener process</h2>

<p>In this post I'd like to show off some fun calculations quantifying interesting properties of the Wiener process, which is probably the simplest continuous-time random walk.</p>

<p>I'm not going to talk about the low-level definition of a Wiener process and I'll take for granted a lot of its basic properties. These include the four properties from its <a href="https://en.wikipedia.org/wiki/Wiener_process">Wikipedia page</a> that uniquely characterize a Wiener process:</p>

<ol>
<li>$W_0 = 0$ almost surely</li>
<li>$W$ has independent increments, so that $W_{t+\Delta t} - W_t$ is independent of $W_s$ for $s\in [0, t]$</li>
<li>$W$ has Gaussian increments, with $W_{t+\Delta t}\sim \mathcal{N}(0,\Delta t)$</li>
<li>$W_t$ is almost surely a continuous function of $t$</li>
</ol>

<p>Another property of the Wiener process that I won't prove is the formula for its <a href="https://en.wikipedia.org/wiki/Quadratic_variation">quadratic variation</a>.</p>

<p>Here are the highlights:</p>

<ul>
<li>Given the start and end values of a Wiener random walk, the intermediate values are Gaussian</li>
<li>The maximum value reached in a fixed interval also has a Gaussian distribution</li>
<li>The time for a Wiener random walk to reach a fixed positive value is LÃ©vy distributed and has no mean</li>
<li>The zeroes of a Wiener random walk are always surrounded arbitrarily closely by other zeroes</li>
<li>Using Riemann sums to approximate the integral of a "nice" function of a Wiener process does not converge <em>that much</em> more slowly than well-behaved functions, despite the spikiness of $W_t$</li>
</ul>

<h3>Interpolating between points on the Wiener process <a id="toc-section-1" class="toc-section"></a></h3>

<p>A natural question: given the values of $W_s$ and $W_t$ for some $t &gt; s &gt; 0$, what distribution of values should we expect at the intermediate points in the interval $(s,t)$?</p>

<p>We can parametrize the most general version of this situation as follows. Let the values of the Wiener process at time $t$ and at time $t + \Delta t_1 + \Delta t_2$ be given, and let time $t + \Delta t_1$ be the time of interest. Rather than talking about the value of the Wiener process in absolute terms, let's consider the <em>differences</em> between its values at these points, letting $\Delta w_1$ and $\Delta w_2$ be the respective changes in the Wiener process over $[t,t+\Delta t_1]$ and $[t+\Delta t_1,t+\Delta t_1 + \Delta t_2]$. This means $\Delta w = \Delta w_1 + \Delta w_2$ is the total change over the interval $[t,t+\Delta t_1 + \Delta t_2]$.</p>

<p>We know that $\Delta w_1 \sim \mathcal{N}(0,\Delta t_1)$ and $\Delta w_2 \sim \mathcal{N}(0, \Delta t_2)$. If the values of $W_t$ and $W_{t+\Delta t_1 + \Delta t_2}$ are given, then their difference is $\Delta w_1 + \Delta w_2$. The value $W_{t+\Delta t_1}$ at the time of interest is simply $W_t + \Delta w_1$. So we can reframe this problem as finding the conditional distribution of $\Delta w_1$ <em>given</em> $\Delta w_1 + \Delta w_2$ i.e. $\Delta w$.</p>

<p>The joint probability density function of $\Delta w_1$ and $\Delta w_2$ just the product of their respective density functions, since they're independent:</p>

<p>
<script type="math/tex; mode=display">f(\Delta w_1, \Delta w_2) = \frac{1}{2\pi \Delta t_1 \Delta t_2}\exp\bigg(-\frac{\Delta w_1^2}{2\Delta t_1^2} -\frac{\Delta w_2^2}{2\Delta t_2^2}\bigg)</script>
</p>

<p>Performing a coordinate transformation, we can obtain the joint density function for $\Delta w_1$ and $\Delta w$. Note that it is <em>not separable</em>, since these two variables aren't independent:</p>

<p>
<script type="math/tex; mode=display">f^\ast(\Delta w_1, \Delta w) = \frac{1}{2\pi \Delta t_1 \Delta t_2}\exp\bigg(-\frac{\Delta w_1^2}{2\Delta t_1^2} -\frac{(\Delta w - \Delta w_1)^2}{2\Delta t_2^2}\bigg)</script>
</p>

<p>This allows us to calculate the conditional density of $\Delta w_1$ as follows:</p>

<p>
<script type="math/tex; mode=display">f^\ast(\Delta w_1 | \Delta w) = \frac{f^\ast(\Delta w_1,\Delta w)}{\int_{\mathbb R} f^\ast(\delta,\Delta w) ~ d\delta} = \frac{1}{\sqrt{2\pi}\Big(\tfrac{1}{\tfrac{1}{\Delta t_1} + \tfrac{1}{\Delta t_2}}\Big)}\cdot \exp\bigg(-\frac{(\Delta w_1 - \tfrac{\Delta t_1 \Delta w}{\Delta t})^2}{2\Big(\tfrac{1}{\tfrac{1}{\Delta t_1} + \tfrac{1}{\Delta t_2}}\Big)^2}\bigg)</script>
</p>

<p>The reason I've written the result in this cumbersome form with all of the messy fractions is to make it clear that this is also a Gaussian distribution in $\Delta w_1$. From how I've written it, you can read off the mean of $\Delta w_1$ as $\Delta t_1 \Delta w/\Delta t$ and its variance as the <em>harmonic sum</em> (see <a href="https://en.wikipedia.org/wiki/Harmonic_mean">harmonic mean</a>) of $\Delta_1$ and $\Delta_2$.</p>

<p>These mean and variance values make intuitive sense, by the way! The mean of $(\Delta t_1/\Delta t) \Delta w$ tells us that the average value of the walk approaches its value at the rightmost endpoint of the interval <em>in direct proportion</em> to how far through the interval it is. For example, halfway between the left and right endpoints, its expectation is just the average of the values at the left and right endpoints. Note also that close to the endpoints, the variance is very small (as we would expect, since the walk has "less time" to deviate from the given fixed values), and in the middle of the interval it is half of the interval's length.</p>

<p>This calculation is very useful if you ever want to simulate a Wiener process in code. The most naive way of implementing a Wiener process would be to generate its values on a fine mesh by repeatedly adding up normally distributed random variables with small variances. But this approach has a big drawback, namely that you have to specify the mesh size in advance. Using this conditional distribution, you can implement a "lazy" Wiener process that only evaluates values on-demand, interpolating between previously-generated values for consistency.</p>

<h3>Time for the walk to reach a threshhold <a id="toc-section-2" class="toc-section"></a></h3>

<p>Another natural question about the Wiener process is "how long before $W_t$ gets bigger than $w$?" This question is about the <a href="https://en.wikipedia.org/wiki/First-hitting-time_model">"first hitting times"</a> for a Wiener process.</p>

<p>There's a really elegant and ingenious way of deriving the distribution of the random variable $T_w$ describing the time at which $W_t$ first reaches the value $w$. The key insight is called the <strong>reflection principle</strong>. By the symmetry of a Wiener process, at any time after a specific time $s$, there is exactly a $50\%$ chance that the random walk will be above $W_s$, and a $50\%$ chance that it will be below $W_s$. That is, for any $0 &lt; s &lt; t$, we have that <script type="math/tex; mode=display">\mathbb P(W_t > W_s) = \frac{1}{2}</script> This has a useful consequence: if the random walk has attained the value $w$ <em>at any time</em> the interval $[0,t)$, then there is a $50\%$ chance that $W_t &gt; w$ and an equal chance that $W_t &lt; w$. Note also that because of the (almost certain) continuity of $W_t$, by the Intermediate Value Theorem, $W_t$ cannot exceed $w$ without first equalling $w$ at some point. Thus <script type="math/tex; mode=display">\mathbb P(W_t > w) = \mathbb P(W_t > w ~ | ~ \exists s\in (0,t) ~ W_s = w) = \frac{1}{2}\mathbb P(s\in (0,t) ~ W_s = w)</script> Implying that the probability of the random walk reaching $w$ somewhere during the interval $(0,t)$ is given by <script type="math/tex; mode=display">2\mathbb P(W_t > w) = 1 - \text{erf}\Big(\frac{w}{\sqrt{2t}}\Big)</script>
</p>

<p>Now let us return to our original question, which is not about the likelihood of reaching $w$ within a certain interval, but rather the distribution of the <em>hitting time</em> for the value $w$. These questions are closely related: the probability that the hitting time $T_w$ lies in the interval $(t, t+\epsilon)$ is precisely the probability that $w$ is achieved within $(0,t+\epsilon)$ but <em>not</em> within $(0,t)$. That is: <script type="math/tex; mode=display">\mathbb P(T_w\in (t,t+\epsilon)) = 2\mathbb P(W_{t+\epsilon} > w) - 2\mathbb P(W_t > w)</script> Hence, the probability density function $f_w(t)$ for $T_w$ is given by <script type="math/tex; mode=display">f_w(t) = \frac{d}{dt}\bigg(1 - \text{erf}\Big(\frac{w}{\sqrt{2t}}\Big)\bigg) = \frac{w}{\sqrt{2\pi t^3}}\exp\Big(-\frac{w^2}{2t}\Big)</script> This kind of distribution is called the <a href="https://en.wikipedia.org/wiki/LÃ©vy_distribution">LÃ©vy distribution</a>. It's interesting to note that this is a "heavy-tailed" distribution, and its mean diverges, so it doesn't make sense to talk about the "average hitting time" for a value $w$.</p>

<h3>Omitting zeroes on an interval <a id="toc-section-3" class="toc-section"></a></h3>

<p>Using what we've calculated in the previous section, we're prepared to answer an interesting question about the zeroes of a Wiener process $W_t$: what is the probability that it will have <em>no zeroes</em> on the interval $[a,b]$?</p>

<p>Given the value $W_a = w$, this question is easy to answer. By symmetry and independent increments, the probability of the random walk failing to cross the x-axis within $(b-a)$ time units starting from $w$ is equal to the probability of a Wiener random walk failing to surpass $w$ within $(b-a)$ time units when starting from a value of $0$. And we have already seen that this probability is equal to <script type="math/tex; mode=display">\text{erf}\Big(\frac{|w|}{\sqrt{2(b-a)}}\Big)</script> But in the question we're asking we are <em>not</em> given the value of $W_a$. We'll have to integrate this expression over all possible values of $W_a$, weighted by their respective probabilities: <script type="math/tex; mode=display">\mathbb P(\forall t\in [a,b] ~ W_t\neq 0) = \int_{\mathbb R} \frac{e^{-w^2/2a}}{\sqrt{2\pi a}} \cdot \text{erf}\Big(\frac{|w|}{\sqrt{2(b-a)}}\Big)dw</script> To evaluate this, let's use the technique of Feynman Integration. If we differentiate under the integral sign with respect to the parameter $b$, the integrand takes on a more familiar form:
<script type="math/tex; mode=display">-\int_{\mathbb R} \frac{e^{-w^2/2a}}{\sqrt{2\pi a}} \cdot \frac{|w|e^{-w^2/2(b-a)}}{\sqrt{2\pi(b-a)^3}}dw</script> The integrand here is antidifferentiable, so the integral can be readily evaluated as <script type="math/tex; mode=display">\frac{1}{\pi}\cdot \frac{\sqrt{a}}{b\sqrt{b-a}}</script> Now, integrating with respect to $b$ and choosing a constant of integration such that the probability is one when $a=b$ (since the probability of $W_t$ having zeroes on $[a,a]$ vanishes), we obtain the final result: <script type="math/tex; mode=display">\frac{2}{\pi} \arctan \frac{1}{\sqrt{\tfrac{b}{a}-1}}</script> There are a couple of "cute" things to notice about this. One of them is that this probability only depends on the ratio $b/a$, a fact which can actually be derived without figuring out an exact formula for the probability using scaling properties of Wiener processes. Another cute fact is that on any interval $[a,2a]$ there is <em>exactly a $50\%$ chance</em> of zeroes occurring!</p>

<h3>First and last zeroes <a id="toc-section-4" class="toc-section"></a></h3>

<p>There's a collection of interesting <a href="https://en.wikipedia.org/wiki/Arcsine_laws_(Wiener_process)">arcsine laws</a> relating to the behaviors of the zeros and maxima of a Wiener process. They say that:</p>

<ol>
<li>The measure of the set ${t\in [0,b] ~ | ~ W_t &gt; 0}$ is arcsine-distributed</li>
<li>The last zero of $W_t$ on $[0,b]$ follows an arcsine distribution</li>
<li>The value of $t\in [0,b]$ for which $W_t$ is greatest follows an arcsine distribution</li>
</ol>

<p>The second law can be derived readily from our previous computation of the probability that $W_t$ is zero-free on $[a,b]$. Let's call this probability $p(a,b)$ for now. The probability that the last zero of $W_t$ on $[0,b]$ lies in the small interval $[a,a+\epsilon)$ is equal to $p(a+\epsilon,b) - p(a,b)$, that is, the probability that $W_t$ is zero-free on $[a+\epsilon, b]$ but not on $[a,b]$. Thus, the density function of the last zero of $W_t$ on $[0,b]$ is given by <script type="math/tex; mode=display">f(a) = \frac{dp}{da} = \frac{1}{\pi}\cdot \frac{1}{a\sqrt{\tfrac{b}{a}-1}}</script> This function has an elementary antiderivative as a function of $a$, so we can integrate to find a cumulative distribution function for the random variable $Z_b$ describing the last zero of $W_t$ on the interval $[0,b]$: <script type="math/tex; mode=display">\mathbb P(Z_b < z) = \frac{2}{\pi}\arcsin\sqrt{\frac{z}{b}}</script> Hence why this law is dubbed an "arcsine law". The third arcsine law, describing the argument of the maximum value attained by $W_t$ on $[0,b]$, can be deduced from this statistic using a more advanced version of the reflection principle (check the Wikipedia page for a sketch). And I still haven't figured out how to prove the first law!</p>

<p>Naturally, you might also ask about the the <em>smallest</em> zero of the Wiener process $W_t$. Of course, the smallest <em>nonnegative</em> zero is at $t=0$, so to make this question nontrivial, we would have to ask about the smallest <em>strictly positive</em> zero.</p>

<p>But it turns out that there (almost certainly) is no smallest positive zero! To be exact - with probability one, there are infinitely many zeroes of $W_t$ in any interval $[0, b]$, no matter the value of $b&gt;0$.</p>

<p>This is quite easy to prove using a fact that we proved in the last section. We showed that on any interval $[a,b]$, the probability of $W_t$ having no zeroes on that interval is <script type="math/tex; mode=display">\frac{2}{\pi} \arctan \frac{1}{\sqrt{\tfrac{b}{a}-1}}</script> Of course, for any $b &gt; a &gt; 0$, the probability of $W_t$ having no zeroes on $(0,b]$ is upper-bounded by the probability of it having no zeroes on $[a,b]$. Thus we have that the probability of $W_t$ having no zeroes on $(0,b]$ is <em>at most</em> <script type="math/tex; mode=display">\lim_{a\to 0^+} \frac{2}{\pi} \arctan \frac{1}{\sqrt{\tfrac{b}{a}-1}}</script> This limit converges to $0$ as $a\to 0^+$, hence the probability of $W_t$ omitting zeroes on $(0,b]$ is precisely zero!</p>

<h3>Convergence rate of Riemann sums <a id="toc-section-5" class="toc-section"></a></h3>

<p>Finally, one more question: when we use Riemann sums to approximate integrals of functions involving Wiener processes, how quickly can we expect them to converge?</p>

<p>For context, when we approximate the integral of a continuously differentiable function over a closed, bounded interval using upper, lower, left or right Riemann sums with $n$ subintervals, we can expect that our approximation will converge to the true value with an error shrinking at a rate of <em>at least</em> $\mathcal O(1/n)$. But with a function that is not continuously differentiable, there is no such guarantee. Mere continuity is enough to guarantee convergence of the Riemann sums, but not necessarily at a rate bounded by $\mathcal O(1/n)$. A classic example is the <a href="https://en.wikipedia.org/wiki/Weierstrass_function">Weierstrass Function</a>, which is continuous but nowhere differentiable. The convergence of its Riemann sums is <a href="https://math.stackexchange.com/questions/3034541/counterexample-to-riemann-sum-limit">strictly slower</a> than $\mathcal O(1/n)$, but it is HÃ¶lder continuous with exponent $1/2$, which means its Riemann sums can be proven to converge at at rate of at least $\mathcal O(1/\sqrt{n})$.</p>

<p>According to the Wikipedia page, a Wiener process is almost surely HÃ¶lder continuous with exponent $1/2 - \epsilon$ for any $\epsilon &gt; 0$. (We won't prove this.) Naively, then, we can <em>at least</em> expect that the Riemann sums of a Wiener process would converge at a rate of at least $\mathcal O(1/\sqrt{n^{1-\epsilon}})$ for any $\epsilon &gt; 0$. Similarly, if $f(x,y)$ is a smooth function $\mathbb R^2\to \mathbb R$, we can expect the Riemann sums for the integral <script type="math/tex; mode=display">\int_0^1 f(t, W_t) ~ dt</script> to converge at this rate as well. But can we hope for anything better?</p>

<p>Surprisingly, the answer is <em>yes</em>! We can actually say that the errors will converge at a rate of $\mathcal O(1/n^{1-\epsilon})$ <em>almost surely</em> (with probability one) for any $\epsilon &gt; 0$, which is nearly as good as the Riemann sum convergence rate for a continuously differentiable function. (Actually, we can do a bit better than this, as we will see.) The "spikiness" of the Wiener makes this argument difficult, but its random up-and-down fluctuations saves the day, allowing us to argue that positive and negative errors will "cancel out" to a sufficient extent on average that we can guarantee the same convergence rate as we could with a continuously differentiable function.</p>

<p>To see why, let's consider left Riemann sums approximating the integral of $f(t,W_t)$ on $[0,1]$. By the smoothness of $f$, for small differences $\Delta t$, we have the following Taylor expansion about $t$: <script type="math/tex; mode=display">f(t+\Delta t,W_{t + \Delta t}) = f(t,W_t) + \frac{\partial f}{\partial t}(t,W_t)\cdot \Delta t + \frac{\partial f}{\partial W} (t,W_t)\cdot \Delta W + \mathcal O(\Delta t^{3/2-\epsilon})</script> where we have made use of the fact that $W_t$ is almost surely HÃ¶lder continuous with exponent $1/2-\epsilon$ to collapse the higher-order terms into the error term $\mathcal O(\Delta t^{3/2-\epsilon})$. This means that the error in the left Riemann sums can be bounded as follows: <script type="math/tex; mode=display">\bigg|\int_0^1 f(t,W_t)dt - \frac{1}{n}\sum_{i=0}^{n-1} f\Big(\frac{i}{n}, W_{i/n}\Big)\bigg| \leq \frac{1}{n}\Big\lVert\frac{\partial f}{\partial t}\Big\rVert_\infty + \bigg|\frac{1}{n}\sum_{i=0}^{n-1} \frac{\partial f}{\partial W}(i/n, W_{i/n})\cdot \Delta W_{i/n}\bigg| + \mathcal O(\Delta t^{3/2-\epsilon})</script> where $\lVert\cdot\rVert_\infty$ represents the supremum, so that $\lVert \partial f/\partial t\rVert_\infty$ is an upper bound on the partial of $f$ with respect to $t$. In the second term of this upper bound, note that each $\Delta W_{i/n}$ is a normally distributed random variable with variance $1/n$, hence each term $i$ of the sum has variance given by <script type="math/tex; mode=display">\frac{1}{n}\bigg|\frac{\partial f}{\partial W}(i/n, W_{i/n})\bigg|\leq \frac{1}{n}\Big\lVert \frac{\partial f}{\partial W}\Big\rVert_\infty</script> Summing all of these $n$ terms gives a normally distributed random variable with overall variance upper-bounded by $\lVert \partial f/\partial W\rVert_\infty$. Call this random variable $E_n$, so that we have <script type="math/tex; mode=display">\bigg|\int_0^1 f(t,W_t)dt - \frac{1}{n}\sum_{i=0}^{n-1} f\Big(\frac{i}{n}, W_{i/n}\Big)\bigg| \leq \frac{1 + |E_n|}{n}\Big\lVert\frac{\partial f}{\partial t}\Big\rVert_\infty +  \mathcal O(\Delta t^{3/2-\epsilon})</script> If $E_n$ is a sequence of normally distributed random variables with bounded variance, then it is not difficult to show that $E_n = \mathcal O(n^\epsilon)$ with probability one for any $\epsilon &gt; 0$. With a bit more care, one can show that $E_n = \mathcal O(\sqrt{\log n})$ with probability one. This tells us that the errors in the left Riemann sums for this integral converge at a rate of $\mathcal O(\sqrt{\log n}/n)$ (or $\mathcal O(1/n^{1-\epsilon})$, if you prefer the weaker estimate) <em>almost certainly</em>.</p>

<p>The key step in this proof is where we used the fact that variance is additive when adding Gaussians together. This distinguishes $W_t$ from other functions that are merely HÃ¶lder continuous. If $w(t)$ is a general function that is HÃ¶lder continuous with exponent $\alpha &lt; 1$, then the most we can say about the sum of the differences $\Delta w(i/n)$ weighted by the values of $\partial f/\partial W$ is that they are bounded by $\sqrt{n}$. But due to the "independent increments" property of a Wiener process, it guarantees that these differences almost certainly cancel sufficiently to bound this sum by $\sqrt{\log n}$.</p>

    <br>
<a href="/">go to homepage</a>
<hr>
<div id="license-statement">The posts on this website are licensed under <a href="https://creativecommons.org/licenses/by-nc/4.0/">CC-by-NC 4.0</a>.</div>
<div id="notbyai"><a href="https://notbyai.fyi/"><img src="/img/written-by-human.png"/><img src="/img/illustrated-by-human.png"/></a></div>
</body>
</html>